###decision tree(决策树)
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
####一、概述
* 优点： 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
* 缺点： 可能会产生过度匹配的问题
* 使用数据类型： 数值型和标称型

####二、算法原理概述
在构造决策树是，需要解决的第一个问题就是：当前数据集上那个特征在划分数据分类时起到决定性的作用。为了找到决定性的特征，划分出最好的结果，必须评估每个特惠增，完成测试之后，原始数据集会被划分成几个数据及，这些数据子集会分布在第一个决策点的所有分支之上。如果某个分支下的数据书友同一类型，则当前已经正确的划分好了这个子集，无须进一步划分。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集，改过成循环执行直到所有具有相同类型的数据均在一个数据子集中
创建分支的伪代码函数 createBranch() 如下所示：

```shell
检测数据集中的每个子项是否属于统一分类：
	if so return 类标签；
	else
		寻找划分数据的最好特征
		划分数据集
		创建分支节点
			for 每个划分的子集
				调用函数createranch并增加返回结果到分支节点中
	return 分支节点
```

####三、决策树的一般流程
1. 收集数据： 可以使用任何方法。
2. 准备数据： 树的构造算法只适用于标称型数据，所以数值型数据必须被离散化。
3. 分析数据： 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
4. 训练算法： 构造树的数据结构。
5. 测试算法： 使用经验树计算错误率。
6. 使用算法： 此步骤可以适用于任何监督学习算法，而使用决策树可以更好的理解数据的内在含义

####四、算法相关知识
* 信息增益
组织杂乱无章的数据的一种方法是使用信息论度量信息，信息论是量化处理信息的分支科学。
在划分数据集之前之后信息发生的变化称为信息增益，直到如何计算信息增益，可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。
集合信息的度量方式称为香浓熵或者简称为熵。
熵定义为信息的期望值：
如果待分类的事务可能划分在多个分类中，则符号$x_i$的信息定义为
$$
l(x_i) = -\log_2p(x_i)
$$
其中p(x_i)是选择该分类的概率
为了计算熵，需要计算所有类别的所有可能值包含的信息期望值，可以通过下面的公式得到：
$$
H = - \sum_{i=1}^{n}p(x_i)\log_2p(x_i)
$$
其中n是分类的个数
